\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Title Page}{}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Certification}{i}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Dedication}{ii}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Acknowledgement}{iii}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Abstract}{iv}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Table of Contents}{v}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Figures}{viii}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{x}{section*.4}\protected@file@percent }
\citation{cooper}
\citation{kaur}
\citation{bayo}
\citation{Knillmann}
\citation{sanborn}
\citation{KIM2017525}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.0}Introduction}{1}{section.1.0}\protected@file@percent }
\citation{Everingham}
\citation{Russakovsky}
\citation{Deng}
\citation{Krizhevsky}
\citation{He2015DeepRL}
\citation{Szegedy}
\citation{Xie}
\citation{Hu2017SqueezeandExcitationN}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Basic Definitions and Results}{3}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Tensor and Vectorisation}{3}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Vector Calculus and the Chain Rule}{4}{subsection.1.1.2}\protected@file@percent }
\newlabel{eqn:2}{{1.1.1}{4}{Vector Calculus and the Chain Rule}{equation.1.1.1}{}}
\newlabel{eqn:3}{{1.1.2}{4}{Vector Calculus and the Chain Rule}{equation.1.1.2}{}}
\newlabel{eqn:4}{{1.1.3}{4}{Vector Calculus and the Chain Rule}{equation.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Binomial Distribution}{4}{subsection.1.1.3}\protected@file@percent }
\newlabel{binomial}{{1.1.4}{4}{Binomial Distribution}{equation.1.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Norms}{5}{subsection.1.1.4}\protected@file@percent }
\newlabel{sub:norm}{{1.1.4}{5}{Norms}{subsection.1.1.4}{}}
\newlabel{eqn:lp-norm}{{1.1.5}{5}{Norms}{equation.1.1.5}{}}
\newlabel{eqn:l2-norm}{{1.1.6}{5}{Norms}{equation.1.1.6}{}}
\newlabel{eqn:l1-norm}{{1.1.7}{5}{Norms}{equation.1.1.7}{}}
\newlabel{eqn:max-norm}{{1.1.8}{6}{Norms}{equation.1.1.8}{}}
\newlabel{eqn:fro-norm}{{1.1.9}{6}{Norms}{equation.1.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Convex Optimisation}{6}{subsection.1.1.5}\protected@file@percent }
\newlabel{eqn:optimize}{{1.1.10}{6}{Convex Optimisation}{equation.1.1.10}{}}
\newlabel{eqn:convex_fxn}{{1.1.11}{7}{}{equation.1.1.11}{}}
\newlabel{eqn:convex_strict}{{1.1.12}{7}{}{equation.1.1.12}{}}
\newlabel{eqn:strong_convex}{{1.1.13}{7}{}{equation.1.1.13}{}}
\newlabel{eqn:subgradient}{{1.1.14}{7}{}{equation.1.1.14}{}}
\newlabel{pro:sum}{{1.1.2}{8}{}{pro.1.1.2}{}}
\newlabel{thm:local_is_global}{{1.1.1}{8}{}{thm.1.1.1}{}}
\newlabel{lemma:sum}{{1.1.2}{8}{}{lemma.1.1.2}{}}
\newlabel{thm:first_order}{{1.1.2}{9}{}{thm.1.1.2}{}}
\newlabel{thm:second_order}{{1.1.3}{9}{}{thm.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.6}Rate of Convergence}{10}{subsection.1.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.7}Logistic Sigmoid Function}{11}{subsection.1.1.7}\protected@file@percent }
\newlabel{sub:lsf}{{1.1.7}{11}{Logistic Sigmoid Function}{subsection.1.1.7}{}}
\newlabel{eqn:sigma}{{1.1.18}{11}{Logistic Sigmoid Function}{equation.1.1.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Logistic sigmoid function\relax }}{11}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sig}{{1.1}{11}{Logistic sigmoid function\relax }{figure.caption.5}{}}
\newlabel{eqn:sigma1}{{1.1.19}{12}{Logistic Sigmoid Function}{equation.1.1.19}{}}
\newlabel{eqn:sigma2}{{1.1.20}{12}{Logistic Sigmoid Function}{equation.1.1.20}{}}
\newlabel{eqn:sigma3}{{1.1.21}{12}{Logistic Sigmoid Function}{equation.1.1.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Machine Learning}{12}{section.1.2}\protected@file@percent }
\citation{Sutton:1998:IRL:551283}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Logistic Regression}{15}{section.1.3}\protected@file@percent }
\newlabel{eqn:train1}{{1.3.1}{15}{Logistic Regression}{equation.1.3.1}{}}
\newlabel{eqn:train2}{{1.3.2}{15}{Logistic Regression}{equation.1.3.2}{}}
\newlabel{eqn:linear}{{1.3.3}{15}{Logistic Regression}{equation.1.3.3}{}}
\newlabel{eq:inference}{{1.3.4}{15}{Logistic Regression}{equation.1.3.4}{}}
\newlabel{eqn:t}{{1.3.5}{15}{Logistic Regression}{equation.1.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Maximum Likelihood Estimation}{15}{section.1.4}\protected@file@percent }
\newlabel{eqn:trial}{{1.4.1}{16}{Maximum Likelihood Estimation}{equation.1.4.1}{}}
\newlabel{eqn:joint}{{1.4.2}{16}{Maximum Likelihood Estimation}{equation.1.4.2}{}}
\newlabel{eqn:arg_max}{{1.4.3}{16}{Maximum Likelihood Estimation}{equation.1.4.3}{}}
\newlabel{eqn:loss}{{1.4.4}{16}{Maximum Likelihood Estimation}{equation.1.4.4}{}}
\newlabel{eqn:loss1}{{1.4.5}{16}{Maximum Likelihood Estimation}{equation.1.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Loss function for $y=0$ and $y=1$\relax }}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:loss}{{1.2}{17}{Loss function for $y=0$ and $y=1$\relax }{figure.caption.6}{}}
\newlabel{eqn:cost}{{1.4.6}{18}{Maximum Likelihood Estimation}{equation.1.4.6}{}}
\newlabel{eqn:dldz}{{1.4.8}{18}{Maximum Likelihood Estimation}{equation.1.4.8}{}}
\newlabel{eqn:djdz}{{1.4.9}{18}{Maximum Likelihood Estimation}{equation.1.4.9}{}}
\newlabel{eqn:hess_w_1}{{1.4.10}{19}{Maximum Likelihood Estimation}{equation.1.4.10}{}}
\newlabel{eqn:hess_b_1}{{1.4.11}{19}{Maximum Likelihood Estimation}{equation.1.4.11}{}}
\newlabel{eqn:positive1}{{1.4.12}{19}{Maximum Likelihood Estimation}{equation.1.4.12}{}}
\newlabel{eqn:positive2}{{1.4.13}{19}{Maximum Likelihood Estimation}{equation.1.4.13}{}}
\newlabel{eqn:hess_w_2}{{1.4.14}{19}{Maximum Likelihood Estimation}{equation.1.4.14}{}}
\citation{dudaHart1973}
\citation{ahs-labm-85}
\newlabel{eqn:hess_b_2}{{1.4.15}{20}{Maximum Likelihood Estimation}{equation.1.4.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Gradient Descent}{20}{section.1.5}\protected@file@percent }
\newlabel{sec:gd}{{1.5}{20}{Gradient Descent}{section.1.5}{}}
\citation{bottou2018optimization}
\newlabel{eqn:gd}{{1.5.1}{21}{Gradient Descent}{equation.1.5.1}{}}
\newlabel{thm:gd_converge}{{1.5.1}{22}{}{thm.1.5.1}{}}
\newlabel{eqn:lipschitz}{{1.5.2}{22}{}{equation.1.5.2}{}}
\newlabel{eqn:sublinear_conv}{{1.5.3}{22}{}{equation.1.5.3}{}}
\newlabel{eqn:a}{{1.5.5}{22}{Gradient Descent}{equation.1.5.5}{}}
\newlabel{eqn:b}{{1.5.6}{23}{Gradient Descent}{equation.1.5.6}{}}
\newlabel{eqn:c}{{1.5.7}{23}{Gradient Descent}{equation.1.5.7}{}}
\newlabel{eqn:d}{{1.5.8}{23}{Gradient Descent}{equation.1.5.8}{}}
\newlabel{eqn:e}{{1.5.9}{23}{Gradient Descent}{equation.1.5.9}{}}
\newlabel{eqn:f}{{1.5.10}{23}{Gradient Descent}{equation.1.5.10}{}}
\citation{bottou2018optimization}
\newlabel{eqn:g}{{1.5.11}{24}{Gradient Descent}{equation.1.5.11}{}}
\newlabel{eqn:linear_conv}{{1.5.15}{24}{}{equation.1.5.15}{}}
\citation{boyd2004convex}
\citation{robbins1951stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Stochastic Gradient Descent}{25}{section.1.6}\protected@file@percent }
\newlabel{sec:sgd}{{1.6}{25}{Stochastic Gradient Descent}{section.1.6}{}}
\newlabel{eqn:sgd_obj}{{1.6.1}{25}{Stochastic Gradient Descent}{equation.1.6.1}{}}
\citation{nemirovski2009robust}
\newlabel{eqn:sgd}{{1.6.2}{26}{Stochastic Gradient Descent}{equation.1.6.2}{}}
\newlabel{eqn:sgd_est}{{1.6.3}{26}{Stochastic Gradient Descent}{equation.1.6.3}{}}
\newlabel{eqn:mini_sgd}{{1.6.4}{26}{Stochastic Gradient Descent}{equation.1.6.4}{}}
\citation{dekel2012optimal}
\citation{10.5555/3086952}
\citation{10.5555/3086952}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Regularisation}{27}{section.1.7}\protected@file@percent }
\newlabel{sec:regularisation}{{1.7}{27}{Regularisation}{section.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Regularisation constant effect on underfitting and overfitting in a model}}{28}{figure.caption.7}\protected@file@percent }
\newlabel{fig:lambda}{{1.3}{28}{Regularisation constant effect on underfitting and overfitting in a model}{figure.caption.7}{}}
\newlabel{eqn:cost_regularisation}{{1.7.1}{29}{Regularisation}{equation.1.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Decision Boundary}{29}{section.1.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Decision boundary for two inputs $\bm  {x}_1$ and $\bm  {x}_2$\relax }}{29}{figure.caption.8}\protected@file@percent }
\newlabel{fig:boundary}{{1.4}{29}{Decision boundary for two inputs $\bm {x}_1$ and $\bm {x}_2$\relax }{figure.caption.8}{}}
\citation{Rosenblatt58theperceptron}
\citation{10.5555/1162264}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{31}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.0}Introduction}{31}{section.2.0}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Deep Learning}{31}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Perceptrons}{31}{subsection.2.1.1}\protected@file@percent }
\citation{MinskyPapert:69}
\citation{RumelhartHintonWilliams86}
\citation{Kelley:1960}
\citation{BrysonHo69}
\citation{Werbos:74}
\citation{7fa6b6a5cde14bcfbd7ab3a8f19d0d56}
\citation{RumelhartHintonWilliams86}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces XOR table\relax }}{32}{table.caption.9}\protected@file@percent }
\newlabel{tab:xor}{{2.1}{32}{XOR table\relax }{table.caption.9}{}}
\citation{Hubel:62}
\citation{fukushima:neocognitronbc}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Back Propagation}{33}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Convolutional Neural Networks}{33}{subsection.2.1.3}\protected@file@percent }
\citation{10.5555/108235.108263}
\citation{NIPS1988_107}
\citation{LeCun:89}
\citation{lecun-gradientbased-learning-applied-1998}
\citation{Werbos:88gasmarket}
\citation{10.1162/neco.1997.9.8.1735}
\citation{10.1162/neco.1997.9.8.1735}
\citation{graves09}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Long Short Term Memory}{34}{subsection.2.1.4}\protected@file@percent }
\citation{gpu2004}
\citation{chellapilla:inria-00112631}
\citation{10.1162/NECO_a_00052}
\citation{Ciresan12multi-columndeep}
\citation{LeCun:89}
\citation{10.5555/1896300.1896315}
\citation{icdar2011Chinese}
\citation{krizhevsky2009learning}
\citation{stallkamp:11}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Advancements in Computing Power}{35}{subsection.2.1.5}\protected@file@percent }
\citation{Silver_2016}
\citation{10.1007/978-3-319-67361-5_18}
\citation{DBLP:journals/corr/Girshick15}
\citation{DBLP:journals/corr/LongSD14}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Deep Learning in Agriculture}{36}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Plant Phenotyping and Yield Prediction}{36}{subsection.2.2.1}\protected@file@percent }
\citation{taghavi:18}
\citation{oliveira2018scalable}
\citation{agronomy9120833}
\citation{syngenta}
\citation{rasmussen2019maize}
\citation{zhang2019deep}
\citation{lin2017focal}
\citation{chen2018automatic}
\citation{sa2018weedmap}
\citation{garcia2017review}
\citation{badrinarayanan2017segnet}
\citation{tetila2019deep}
\citation{Szegedy}
\citation{He2015DeepRL}
\citation{huang2017densely}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Weed Management and Pest Control}{38}{subsection.2.2.2}\protected@file@percent }
\citation{yu2019deep}
\citation{tao2016detectnet}
\citation{simonyan2014very}
\citation{prasanna2016using}
\citation{Krizhevsky}
\citation{szegedy2015going}
\citation{sladojevic2016deep}
\citation{ferentinos2018deep}
\citation{krizhevsky2014one}
\citation{sermanet2013overfeat}
\citation{simonyan2014very}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Crop Diseases}{39}{subsection.2.2.3}\protected@file@percent }
\citation{chen2019visual}
\citation{barre2017leafnet}
\citation{McCulloch1943}
\citation{Rosenblatt58theperceptron}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Neural Networks}{41}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.0}Introduction}{41}{section.3.0}\protected@file@percent }
\newlabel{z1}{{3.0.1}{41}{Introduction}{equation.3.0.1}{}}
\newlabel{a1}{{3.0.2}{42}{Introduction}{equation.3.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Network diagram for one hidden layer neural network\relax }}{42}{figure.caption.10}\protected@file@percent }
\newlabel{fig:hidden}{{3.1}{42}{Network diagram for one hidden layer neural network\relax }{figure.caption.10}{}}
\newlabel{z2}{{3.0.3}{42}{Introduction}{equation.3.0.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Activation Functions}{44}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Logistic Sigmoid Function}{44}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Gradient of logistic sigmoid function\relax }}{45}{figure.caption.11}\protected@file@percent }
\newlabel{fig:sig_prime}{{3.2}{45}{Gradient of logistic sigmoid function\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Hyperpoblic Tangent Function}{45}{subsection.3.1.2}\protected@file@percent }
\newlabel{eqn:tanh}{{3.1.1}{45}{Hyperpoblic Tangent Function}{equation.3.1.1}{}}
\newlabel{eqn:tanh_prime}{{3.1.2}{45}{Hyperpoblic Tangent Function}{equation.3.1.2}{}}
\citation{10.5555/3086952}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces $\qopname  \relax o{tanh}$ function\relax }}{46}{figure.caption.12}\protected@file@percent }
\newlabel{fig:tanh}{{3.3}{46}{$\tanh $ function\relax }{figure.caption.12}{}}
\citation{10.5555/3086952}
\citation{Hahnloser}
\citation{Jarrett}
\citation{10.5555/3104322.3104425}
\citation{Krizhevsky}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Gradient of $\qopname  \relax o{tanh}$ function\relax }}{47}{figure.caption.13}\protected@file@percent }
\newlabel{fig:tanh_prime}{{3.4}{47}{Gradient of $\tanh $ function\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Rectified Linear Activation Function}{48}{subsection.3.1.3}\protected@file@percent }
\newlabel{eqn:relu}{{3.1.3}{48}{Rectified Linear Activation Function}{equation.3.1.3}{}}
\citation{10.5555/3086952}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces ReLU function\relax }}{49}{figure.caption.14}\protected@file@percent }
\newlabel{fig:relu}{{3.5}{49}{ReLU function\relax }{figure.caption.14}{}}
\newlabel{eqn:relu_prime}{{3.1.4}{50}{Rectified Linear Activation Function}{equation.3.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Gradient of ReLU\relax }}{50}{figure.caption.15}\protected@file@percent }
\newlabel{fig:relu_prime}{{3.6}{50}{Gradient of ReLU\relax }{figure.caption.15}{}}
\citation{Maas2013}
\citation{10.1109/ICCV.2015.123}
\citation{10.5555/3104322.3104425}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces ReLU and some of its common variations\relax }}{51}{figure.caption.16}\protected@file@percent }
\newlabel{fig:compare}{{3.7}{51}{ReLU and some of its common variations\relax }{figure.caption.16}{}}
\newlabel{eqn:lrelu}{{3.1.5}{51}{Rectified Linear Activation Function}{equation.3.1.5}{}}
\citation{Clevert2015FastAA}
\citation{DBLP:journals/corr/KlambauerUMH17}
\citation{DBLP:journals/corr/abs-1710-05941}
\newlabel{eqn:elu}{{3.1.7}{52}{Rectified Linear Activation Function}{equation.3.1.7}{}}
\newlabel{eqn:selu}{{3.1.8}{52}{Rectified Linear Activation Function}{equation.3.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Swish Activation Function}{52}{subsection.3.1.4}\protected@file@percent }
\newlabel{eqn:swish}{{3.1.9}{52}{Swish Activation Function}{equation.3.1.9}{}}
\newlabel{eqn:swish_prime}{{3.1.10}{52}{Swish Activation Function}{equation.3.1.10}{}}
\citation{DBLP:journals/corr/abs-1710-05941}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Swish function\relax }}{53}{figure.caption.17}\protected@file@percent }
\newlabel{fig:swish}{{3.8}{53}{Swish function\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Gradient of swish function\relax }}{54}{figure.caption.18}\protected@file@percent }
\newlabel{fig:swish_prime}{{3.9}{54}{Gradient of swish function\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Softmax Function}{55}{subsection.3.1.5}\protected@file@percent }
\newlabel{eqn:softmax}{{3.1.11}{55}{Softmax Function}{equation.3.1.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Backpropagation}{55}{section.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Forward and backward propagation in a neural network layer\relax }}{56}{figure.caption.19}\protected@file@percent }
\newlabel{fig:wahala}{{3.10}{56}{Forward and backward propagation in a neural network layer\relax }{figure.caption.19}{}}
\newlabel{eqn:train3}{{3.2.1}{57}{Backpropagation}{equation.3.2.1}{}}
\newlabel{eqn:train4}{{3.2.2}{57}{Backpropagation}{equation.3.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Random Initialisation}{58}{subsection.3.2.1}\protected@file@percent }
\citation{10.1109/ICCV.2015.123}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Convolutional Neural Networks}{59}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Convolution Layer}{60}{subsection.3.3.1}\protected@file@percent }
\newlabel{subsec:conv_layer}{{3.3.1}{60}{Convolution Layer}{subsection.3.3.1}{}}
\newlabel{eqn:conv}{{3.3.1}{60}{Convolution Layer}{equation.3.3.1}{}}
\newlabel{eqn:2d_conv}{{3.3.2}{60}{Convolution Layer}{equation.3.3.2}{}}
\newlabel{eqn:2d_cross}{{3.3.3}{60}{Convolution Layer}{equation.3.3.3}{}}
\citation{10.5555/3086952}
\citation{10.5555/3086952}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Two-dimensional cross-correlation operation}}{61}{figure.caption.20}\protected@file@percent }
\newlabel{fig:cross_cor}{{3.11}{61}{Two-dimensional cross-correlation operation}{figure.caption.20}{}}
\newlabel{subfig:sparse1}{{3.12a}{61}{When $\bm {s}$ is formed by convolution with a kernel of width $3$, only three outputs are affected by $\bm {x}$\relax }{figure.caption.21}{}}
\newlabel{sub@subfig:sparse1}{{a}{61}{When $\bm {s}$ is formed by convolution with a kernel of width $3$, only three outputs are affected by $\bm {x}$\relax }{figure.caption.21}{}}
\newlabel{subfig:sparse2}{{3.12b}{61}{When $\bm {s}$ is formed by matrix multiplication, connectivity is no longer sparse, so all the outputs are affected by $\bm {x}_3$\relax }{figure.caption.21}{}}
\newlabel{sub@subfig:sparse2}{{b}{61}{When $\bm {s}$ is formed by matrix multiplication, connectivity is no longer sparse, so all the outputs are affected by $\bm {x}_3$\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Representation of sparse and full connectivity}}{61}{figure.caption.21}\protected@file@percent }
\newlabel{fig:sparse}{{3.12}{61}{Representation of sparse and full connectivity}{figure.caption.21}{}}
\citation{sobel19683x3}
\citation{jaehne1999k}
\newlabel{subfig:detection}{{3.13a}{62}{Cross-correlating a vertical edge filter on an input image\relax }{figure.caption.22}{}}
\newlabel{sub@subfig:detection}{{a}{62}{Cross-correlating a vertical edge filter on an input image\relax }{figure.caption.22}{}}
\newlabel{subfig:detection1}{{3.13b}{62}{Picture plots where bright halves gives brighter pixel intensity values, and darker halves gives darker pixel intensity values\relax }{figure.caption.22}{}}
\newlabel{sub@subfig:detection1}{{b}{62}{Picture plots where bright halves gives brighter pixel intensity values, and darker halves gives darker pixel intensity values\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Vertical edge detection\relax }}{62}{figure.caption.22}\protected@file@percent }
\newlabel{fig:detection}{{3.13}{62}{Vertical edge detection\relax }{figure.caption.22}{}}
\citation{zhang2020dive}
\citation{zhang2020dive}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Padding and Stride}{63}{subsection.3.3.2}\protected@file@percent }
\citation{zhang2020dive}
\citation{zhang2020dive}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Two-dimensional cross-correlation with padding}}{64}{figure.caption.23}\protected@file@percent }
\newlabel{fig:pad}{{3.14}{64}{Two-dimensional cross-correlation with padding}{figure.caption.23}{}}
\newlabel{eqn:pad}{{3.3.4}{64}{Padding and Stride}{equation.3.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Cross-correlation with strides of $3$ and $2$ for height and width respectively}}{65}{figure.caption.24}\protected@file@percent }
\newlabel{fig:stride}{{3.15}{65}{Cross-correlation with strides of $3$ and $2$ for height and width respectively}{figure.caption.24}{}}
\newlabel{eqn:pad_stride}{{3.3.5}{65}{Padding and Stride}{equation.3.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Multiple Input and Multiple Output Channels}{65}{subsection.3.3.3}\protected@file@percent }
\citation{zhang2020dive}
\citation{zhang2020dive}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Cross-correlation computation with $2$ input channels}}{66}{figure.caption.25}\protected@file@percent }
\newlabel{fig:2-dim}{{3.16}{66}{Cross-correlation computation with $2$ input channels}{figure.caption.25}{}}
\citation{zhang2020dive}
\citation{zhang2020dive}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Pooling Layer}{67}{subsection.3.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Maximum pooling with a pooling window shape of $2 \times 2$}}{67}{figure.caption.26}\protected@file@percent }
\newlabel{fig:pool}{{3.17}{67}{Maximum pooling with a pooling window shape of $2 \times 2$}{figure.caption.26}{}}
\citation{vsrivastava14a}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Dropout}{68}{section.3.4}\protected@file@percent }
\citation{zhang2020dive}
\citation{zhang2020dive}
\citation{ioffe15}
\newlabel{eqn:dropout}{{3.4.1}{69}{Dropout}{equation.3.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Fully connected layer before and after dropout}}{69}{figure.caption.27}\protected@file@percent }
\newlabel{fig:dropout}{{3.18}{69}{Fully connected layer before and after dropout}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Batch Normalisation}{69}{section.3.5}\protected@file@percent }
\newlabel{eqn:batch_norm}{{3.5.1}{70}{Batch Normalisation}{equation.3.5.1}{}}
\newlabel{eqn:mean_variance}{{3.5.2}{70}{Batch Normalisation}{equation.3.5.2}{}}
\citation{dauphin17e}
\citation{qian1999momentum}
\citation{nesterov1983method}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Gradient Descent Optimisation Algorithms}{71}{section.3.6}\protected@file@percent }
\newlabel{eqn:momentum}{{3.6.1}{71}{Gradient Descent Optimisation Algorithms}{equation.3.6.1}{}}
\citation{duchi2011adaptive}
\newlabel{eqn:nesterov}{{3.6.2}{72}{Gradient Descent Optimisation Algorithms}{equation.3.6.2}{}}
\newlabel{eqn:nesterov_e}{{3.6.3}{72}{Gradient Descent Optimisation Algorithms}{equation.3.6.3}{}}
\newlabel{eqn:adagrad_partial}{{3.6.4}{72}{Gradient Descent Optimisation Algorithms}{equation.3.6.4}{}}
\newlabel{eqn:adagrad_sgd}{{3.6.5}{72}{Gradient Descent Optimisation Algorithms}{equation.3.6.5}{}}
\citation{kingma2014adam}
\newlabel{eqn:adagrad_update}{{3.6.6}{73}{Gradient Descent Optimisation Algorithms}{equation.3.6.6}{}}
\newlabel{eqn:adagrad}{{3.6.7}{73}{Gradient Descent Optimisation Algorithms}{equation.3.6.7}{}}
\newlabel{eqn:adam_mean}{{3.6.8}{73}{Gradient Descent Optimisation Algorithms}{equation.3.6.8}{}}
\newlabel{eqn:adam_bias}{{3.6.9}{73}{Gradient Descent Optimisation Algorithms}{equation.3.6.9}{}}
\newlabel{eqn:adam}{{3.6.10}{73}{Gradient Descent Optimisation Algorithms}{equation.3.6.10}{}}
\citation{stephen2019efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Plant Disease Detection}{75}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.0}Problem Statement and Dataset}{75}{section.4.0}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The PlantVillage image dataset. This dataset contains 38 categories of diseased or healthy leaf images\relax }}{75}{figure.caption.28}\protected@file@percent }
\newlabel{fig:data}{{4.1}{75}{The PlantVillage image dataset. This dataset contains 38 categories of diseased or healthy leaf images\relax }{figure.caption.28}{}}
\citation{Howard_2020}
\citation{NEURIPS2019_9015}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Bar chart showing the number of images in each class\relax }}{76}{figure.caption.29}\protected@file@percent }
\newlabel{fig:count}{{4.2}{76}{Bar chart showing the number of images in each class\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Model Training}{77}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Base Model Architecture}{77}{section.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Simplified base model architecture\relax }}{77}{figure.caption.30}\protected@file@percent }
\newlabel{fig:cnn_arch}{{4.3}{77}{Simplified base model architecture\relax }{figure.caption.30}{}}
\gdef \LT@i {\LT@entry 
    {1}{58.18805pt}\LT@entry 
    {1}{180.75945pt}\LT@entry 
    {1}{108.07115pt}\LT@entry 
    {1}{93.40454pt}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Base model architecture\relax }}{78}{table.caption.31}\protected@file@percent }
\newlabel{tab:cnn_arch}{{4.2}{78}{Base model architecture\relax }{table.caption.31}{}}
\citation{simonyan2015deep}
\citation{Krizhevsky}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Transfer Learning}{79}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}VGG16}{79}{subsection.4.3.1}\protected@file@percent }
\citation{Das2019DoubleCV}
\citation{Das2019DoubleCV}
\citation{He2015DeepRL}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces VGG16 model architecture}}{80}{figure.caption.32}\protected@file@percent }
\newlabel{fig:vgg16_arch}{{4.4}{80}{VGG16 model architecture}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}ResNext-50}{80}{subsection.4.3.2}\protected@file@percent }
\citation{He2015DeepRL}
\citation{He2015DeepRL}
\citation{He2015DeepRL}
\citation{szegedy2015going}
\newlabel{subfig:residual_block}{{4.5a}{81}{A residual block\relax }{figure.caption.33}{}}
\newlabel{sub@subfig:residual_block}{{a}{81}{A residual block\relax }{figure.caption.33}{}}
\newlabel{subfig:bottleneck}{{4.5b}{81}{A bottleneck building block for ResNet-50/101/152\relax }{figure.caption.33}{}}
\newlabel{sub@subfig:bottleneck}{{b}{81}{A bottleneck building block for ResNet-50/101/152\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Residual learning blocks}}{81}{figure.caption.33}\protected@file@percent }
\newlabel{fig:residual_learning}{{4.5}{81}{Residual learning blocks}{figure.caption.33}{}}
\citation{zagoruyko2017wide}
\citation{Xie}
\citation{Xie}
\citation{Xie}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces A block of ResNeXt with cardinality = 32}}{82}{figure.caption.34}\protected@file@percent }
\newlabel{fig:resnext}{{4.6}{82}{A block of ResNeXt with cardinality = 32}{figure.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Results}{83}{section.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Confusion matrix for $4$-class classification}}{83}{table.caption.35}\protected@file@percent }
\newlabel{tab:confusion}{{4.3}{83}{Confusion matrix for $4$-class classification}{table.caption.35}{}}
\newlabel{eqn:acccuracy}{{4.4.1}{83}{Results}{equation.4.4.1}{}}
\newlabel{eqn:precision}{{4.4.2}{84}{Results}{equation.4.4.2}{}}
\newlabel{eqn:recall}{{4.4.3}{84}{Results}{equation.4.4.3}{}}
\newlabel{eqn:f1-score}{{4.4.4}{84}{Results}{equation.4.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Comparing different evaluation metrics for the models studied on validation data\relax }}{84}{table.caption.36}\protected@file@percent }
\newlabel{tab:result}{{4.4}{84}{Comparing different evaluation metrics for the models studied on validation data\relax }{table.caption.36}{}}
\newlabel{subfig:base_loss}{{4.7a}{85}{Base model training loss\relax }{figure.caption.37}{}}
\newlabel{sub@subfig:base_loss}{{a}{85}{Base model training loss\relax }{figure.caption.37}{}}
\newlabel{subfig:vgg_loss}{{4.7b}{85}{VGG16 training loss\relax }{figure.caption.37}{}}
\newlabel{sub@subfig:vgg_loss}{{b}{85}{VGG16 training loss\relax }{figure.caption.37}{}}
\newlabel{subfig:resnext_loss}{{4.7c}{85}{ResNext-50 training loss\relax }{figure.caption.37}{}}
\newlabel{sub@subfig:resnext_loss}{{c}{85}{ResNext-50 training loss\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Training loss for different models}}{85}{figure.caption.37}\protected@file@percent }
\newlabel{fig:models_loss}{{4.7}{85}{Training loss for different models}{figure.caption.37}{}}
\newlabel{subfig:base_confusion}{{4.8a}{86}{Base model confusion matrix\relax }{figure.caption.38}{}}
\newlabel{sub@subfig:base_confusion}{{a}{86}{Base model confusion matrix\relax }{figure.caption.38}{}}
\newlabel{subfig:vgg_confusion}{{4.8b}{86}{VGG16 confusion matrix\relax }{figure.caption.38}{}}
\newlabel{sub@subfig:vgg_confusion}{{b}{86}{VGG16 confusion matrix\relax }{figure.caption.38}{}}
\newlabel{subfig:resnext_confusion}{{4.8c}{86}{ResNext-50 confusion matrix\relax }{figure.caption.38}{}}
\newlabel{sub@subfig:resnext_confusion}{{c}{86}{ResNext-50 confusion matrix\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Confusion matrix for different models}}{86}{figure.caption.38}\protected@file@percent }
\newlabel{fig:confusion_matrix}{{4.8}{86}{Confusion matrix for different models}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{87}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{apalike}
\bibdata{alliref}
\bibcite{ahs-labm-85}{{1}{1985}{{Ackley et~al.}}{{}}}
\bibcite{badrinarayanan2017segnet}{{2}{2017}{{Badrinarayanan et~al.}}{{}}}
\bibcite{barre2017leafnet}{{3}{2017}{{Barr{\'e} et~al.}}{{}}}
\bibcite{10.1007/978-3-319-67361-5_18}{{4}{2018}{{Baweja et~al.}}{{}}}
\bibcite{behera2019performance}{{5}{2019}{{Behera et~al.}}{{}}}
\bibcite{10.5555/1162264}{{6}{2006}{{Bishop}}{{}}}
\bibcite{bottou2018optimization}{{7}{2018}{{Bottou et~al.}}{{}}}
\bibcite{boyd2004convex}{{8}{2004}{{Boyd et~al.}}{{}}}
\bibcite{BrysonHo69}{{9}{1969}{{Bryson and Ho}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{89}{chapter.5}\protected@file@percent }
\bibcite{chellapilla:inria-00112631}{{10}{2006}{{Chellapilla et~al.}}{{}}}
\bibcite{chen2018automatic}{{11}{2018}{{Chen et~al.}}{{}}}
\bibcite{chen2019visual}{{12}{2019}{{Chen et~al.}}{{}}}
\bibcite{Ciresan12multi-columndeep}{{13}{2012}{{Ciregan et~al.}}{{}}}
\bibcite{10.1162/NECO_a_00052}{{14}{2010}{{Ciresan et~al.}}{{}}}
\bibcite{Clevert2015FastAA}{{15}{2015}{{Clevert et~al.}}{{}}}
\bibcite{cooper}{{16}{2007}{{Cooper and Dobson}}{{}}}
\bibcite{Das2019DoubleCV}{{17}{2019}{{Das et~al.}}{{}}}
\bibcite{dauphin17e}{{18}{2014}{{Dauphin et~al.}}{{}}}
\bibcite{dekel2012optimal}{{19}{2012}{{Dekel et~al.}}{{}}}
\bibcite{Deng}{{20}{2009}{{Deng et~al.}}{{}}}
\bibcite{NIPS1988_107}{{21}{1989}{{Denker et~al.}}{{}}}
\bibcite{duchi2011adaptive}{{22}{2011}{{Duchi et~al.}}{{}}}
\bibcite{dudaHart1973}{{23}{1973}{{Duda and Hart}}{{}}}
\bibcite{Everingham}{{24}{2010}{{Everingham et~al.}}{{}}}
\bibcite{ferentinos2018deep}{{25}{2018}{{Ferentinos}}{{}}}
\bibcite{fukushima:neocognitronbc}{{26}{1980}{{Fukushima}}{{}}}
\bibcite{garcia2017review}{{27}{2017}{{Garcia-Garcia et~al.}}{{}}}
\bibcite{guarav}{{28}{2020}{{Gaurav Belani}}{{}}}
\bibcite{DBLP:journals/corr/Girshick15}{{29}{2015}{{Girshick}}{{}}}
\bibcite{pmlr-v15-glorot11a}{{30}{2011}{{Glorot et~al.}}{{}}}
\bibcite{10.5555/3086952}{{31}{2016}{{Goodfellow et~al.}}{{}}}
\bibcite{graves09}{{32}{2009}{{Graves et~al.}}{{}}}
\bibcite{Hahnloser}{{33}{2000}{{Hahnloser et~al.}}{{}}}
\bibcite{He2015DeepRL}{{34}{2015a}{{He et~al.}}{{}}}
\bibcite{10.1109/ICCV.2015.123}{{35}{2015b}{{He et~al.}}{{}}}
\bibcite{10.1162/neco.1997.9.8.1735}{{36}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{Howard_2020}{{37}{2020}{{Howard and Gugger}}{{}}}
\bibcite{Hu2017SqueezeandExcitationN}{{38}{2017}{{Hu et~al.}}{{}}}
\bibcite{huang2017densely}{{39}{2017}{{Huang et~al.}}{{}}}
\bibcite{Hubel:62}{{40}{1962}{{Hubel and Wiesel}}{{}}}
\bibcite{ioffe15}{{41}{2015}{{Ioffe and Szegedy}}{{}}}
\bibcite{jaehne1999k}{{42}{1999}{{J{\"a}hne et~al.}}{{}}}
\bibcite{Jarrett}{{43}{2009}{{Jarrett et~al.}}{{}}}
\bibcite{kaur}{{44}{2014}{{Kaur and Garg}}{{}}}
\bibcite{Kelley:1960}{{45}{1960}{{Kelley}}{{}}}
\bibcite{agronomy9120833}{{46}{2019}{{Khaki et~al.}}{{}}}
\bibcite{KIM2017525}{{47}{2017}{{Kim et~al.}}{{}}}
\bibcite{kingma2014adam}{{48}{2014}{{Kingma and Ba}}{{}}}
\bibcite{DBLP:journals/corr/KlambauerUMH17}{{49}{2017}{{Klambauer et~al.}}{{}}}
\bibcite{Knillmann}{{50}{2019}{{Knillmann and Liess}}{{}}}
\bibcite{krizhevsky2014one}{{51}{2014}{{Krizhevsky}}{{}}}
\bibcite{krizhevsky2009learning}{{52}{2009}{{Krizhevsky et~al.}}{{}}}
\bibcite{Krizhevsky}{{53}{2012}{{Krizhevsky et~al.}}{{}}}
\bibcite{7fa6b6a5cde14bcfbd7ab3a8f19d0d56}{{54}{1985}{{LeCun}}{{}}}
\bibcite{LeCun:89}{{55}{1989}{{LeCun et~al.}}{{}}}
\bibcite{lecun-gradientbased-learning-applied-1998}{{56}{1998}{{LeCun et~al.}}{{}}}
\bibcite{10.5555/1896300.1896315}{{57}{2004}{{LeCun et~al.}}{{}}}
\bibcite{lin2017focal}{{58}{2017}{{Lin et~al.}}{{}}}
\bibcite{icdar2011Chinese}{{59}{2011}{{Liu et~al.}}{{}}}
\bibcite{DBLP:journals/corr/LongSD14}{{60}{2014}{{Long et~al.}}{{}}}
\bibcite{Maas2013}{{61}{2013}{{Maas et~al.}}{{}}}
\bibcite{McCulloch1943}{{62}{1943}{{McCulloch and Pitts}}{{}}}
\bibcite{MinskyPapert:69}{{63}{1969}{{Minsky and Papert}}{{}}}
\bibcite{prasanna2016using}{{64}{2016}{{Mohanty et~al.}}{{}}}
\bibcite{10.5555/3104322.3104425}{{65}{2010}{{Nair and Hinton}}{{}}}
\bibcite{nemirovski2009robust}{{66}{2009}{{Nemirovski et~al.}}{{}}}
\bibcite{nesterov1983method}{{67}{1983}{{Nesterov}}{{}}}
\bibcite{gpu2004}{{68}{2004}{{Oh and Jung}}{{}}}
\bibcite{oliveira2018scalable}{{69}{2018}{{Oliveira et~al.}}{{}}}
\bibcite{MatrixCalculus}{{70}{2018}{{Parr and Howard}}{{}}}
\bibcite{NEURIPS2019_9015}{{71}{2019}{{Paszke et~al.}}{{}}}
\bibcite{qian1999momentum}{{72}{1999}{{Qian}}{{}}}
\bibcite{DBLP:journals/corr/abs-1710-05941}{{73}{2017}{{Ramachandran et~al.}}{{}}}
\bibcite{rasmussen2019maize}{{74}{2019}{{Rasmussen and Moeslund}}{{}}}
\bibcite{robbins1951stochastic}{{75}{1951}{{Robbins and Monro}}{{}}}
\bibcite{Rosenblatt58theperceptron}{{76}{1958}{{Rosenblatt}}{{}}}
\bibcite{RumelhartHintonWilliams86}{{77}{1986}{{Rumelhart et~al.}}{{}}}
\bibcite{Russakovsky}{{78}{2014}{{Russakovsky et~al.}}{{}}}
\bibcite{sa2018weedmap}{{79}{2018}{{Sa et~al.}}{{}}}
\bibcite{sanborn}{{80}{2007}{{Sanborn et~al.}}{{}}}
\bibcite{bayo}{{81}{2014}{{Sanchez-Bayo and Goka}}{{}}}
\bibcite{sermanet2013overfeat}{{82}{2013}{{Sermanet et~al.}}{{}}}
\bibcite{Silver_2016}{{83}{2016}{{Silver et~al.}}{{}}}
\bibcite{simonyan2014very}{{84}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{simonyan2015deep}{{85}{2015}{{Simonyan and Zisserman}}{{}}}
\bibcite{sladojevic2016deep}{{86}{2016}{{Sladojevic et~al.}}{{}}}
\bibcite{sobel19683x3}{{87}{1968}{{Sobel and Feldman}}{{}}}
\bibcite{vsrivastava14a}{{88}{2014}{{Srivastava et~al.}}{{}}}
\bibcite{stallkamp:11}{{89}{2011}{{Stallkamp et~al.}}{{}}}
\bibcite{stephen2019efficient}{{90}{2019}{{Stephen et~al.}}{{}}}
\bibcite{Sutton:1998:IRL:551283}{{91}{1998}{{Sutton and Barto}}{{}}}
\bibcite{syngenta}{{92}{2019}{{Syngenta}}{{}}}
\bibcite{szegedy2015going}{{93}{2015}{{Szegedy et~al.}}{{}}}
\bibcite{Szegedy}{{94}{2016}{{Szegedy et~al.}}{{}}}
\bibcite{taghavi:18}{{95}{2018}{{Taghavi et~al.}}{{}}}
\bibcite{tao2016detectnet}{{96}{2016}{{Tao et~al.}}{{}}}
\bibcite{tetila2019deep}{{97}{2019}{{Tetila et~al.}}{{}}}
\bibcite{10.5555/108235.108263}{{98}{1990}{{Waibel et~al.}}{{}}}
\bibcite{Werbos:74}{{99}{1974}{{Werbos}}{{}}}
\bibcite{Werbos:88gasmarket}{{100}{1988}{{Werbos}}{{}}}
\bibcite{Xie}{{101}{2017}{{Xie et~al.}}{{}}}
\bibcite{yu2019deep}{{102}{2019}{{Yu et~al.}}{{}}}
\bibcite{zagoruyko2017wide}{{103}{2017}{{Zagoruyko and Komodakis}}{{}}}
\bibcite{zhang2020dive}{{104}{2020}{{Zhang et~al.}}{{}}}
\bibcite{zhang2019deep}{{105}{2019}{{Zhang}}{{}}}
\citation{MatrixCalculus}
\citation{guarav}
\citation{pmlr-v15-glorot11a}
\citation{10.5555/1162264}
\citation{zhang2020dive}
\citation{behera2019performance}
\gdef \@abspage@last{110}
